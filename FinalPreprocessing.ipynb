{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "import joblib\n",
        "import os # Import os to join paths"
      ],
      "metadata": {
        "id": "9w1mBdqgbCDY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. Setup ---\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# --- 1a. Mount Google Drive (for Colab) ---\n",
        "print(\"Mounting Google Drive...\")\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    pass\n",
        "\n",
        "# --- 1b. Constants ---\n",
        "DRIVE_MOUNT_POINT = '/content/drive/MyDrive/'\n",
        "# Define your project folder\n",
        "PROJECT_DIR = os.path.join(DRIVE_MOUNT_POINT, 'shodhAI')\n",
        "\n",
        "# Update FILE_PATH to point to your shodhAI folder\n",
        "FILE_PATH = os.path.join(PROJECT_DIR, 'accepted_2007_to_2018Q4.csv.gz')\n",
        "# Update SAVE_DIR to save processed files in the shodhAI folder\n",
        "SAVE_DIR = PROJECT_DIR\n",
        "SAMPLE_ROWS = 200000\n",
        "\n",
        "DEFAULT_STATUSES = ['Charged Off', 'Default', 'Does not meet the credit policy. Status:Charged Off']\n",
        "PAID_STATUSES = ['Fully Paid', 'Does not meet the credit policy. Status:Fully Paid']\n",
        "TERMINAL_STATUSES = DEFAULT_STATUSES + PAID_STATUSES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfnkGagVbGYJ",
        "outputId": "5c883c16-84f6-480e-f430-2301b138723c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Feature & Target Definitions ---\n",
        "# Based on our EDA in task_1_eda.py\n",
        "NUMERIC_FEATURES = [\n",
        "    'loan_amnt', 'int_rate', 'annual_inc', 'dti',\n",
        "    'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc'\n",
        "]\n",
        "CATEGORICAL_FEATURES = [\n",
        "    'grade', 'emp_length', 'home_ownership', 'verification_status'\n",
        "]\n",
        "# All features our model will use\n",
        "ALL_FEATURES = NUMERIC_FEATURES + CATEGORICAL_FEATURES\n",
        "# Features needed for reward calculation\n",
        "REWARD_FEATURES = ['loan_amnt', 'int_rate']\n",
        "\n",
        "# --- 3. Load Data ---\n",
        "print(f\"Loading {SAMPLE_ROWS} rows from {FILE_PATH}...\")\n",
        "try:\n",
        "    df = pd.read_csv(\n",
        "        FILE_PATH,\n",
        "        nrows=SAMPLE_ROWS,\n",
        "        compression='gzip',\n",
        "        usecols=ALL_FEATURES + REWARD_FEATURES + ['loan_status'],\n",
        "        low_memory=False\n",
        "    )\n",
        "    print(f\"Successfully loaded {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {FILE_PATH}\")\n",
        "    print(\"Please double-check the FILE_PATH variable.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred loading data: {e}\")\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV2fZKZYbGWG",
        "outputId": "5257b5bf-16bc-436c-bc46-86121f06f16f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 200000 rows from /content/drive/MyDrive/shodhAI/accepted_2007_to_2018Q4.csv.gz...\n",
            "Successfully loaded 200000 rows and 14 columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Initial Cleaning & Filtering ---\n",
        "print(\"Filtering for terminal loan statuses...\")\n",
        "df_filtered = df[df['loan_status'].isin(TERMINAL_STATUSES)].copy()\n",
        "print(f\"Filtered size (terminal loans only): {df_filtered.shape[0]} rows\")\n",
        "\n",
        "if df_filtered.empty:\n",
        "    print(\"\\nWarning: No terminal-status loans found. Try increasing SAMPLE_ROWS.\")\n",
        "    exit()\n",
        "\n",
        "# --- 5. Target and Reward Engineering ---\n",
        "print(\"Engineering Target (y) and Reward (r) variables...\")\n",
        "\n",
        "# 5.1. Target (y) for Task 2 (DL Model)\n",
        "# {0: Fully Paid, 1: Defaulted}\n",
        "df_filtered['target'] = df_filtered['loan_status'].apply(lambda x: 1 if x in DEFAULT_STATUSES else 0)\n",
        "\n",
        "# 5.2. Reward (r) for Task 3 (RL Model)\n",
        "# Clean int_rate for calculation\n",
        "df_filtered['int_rate'] = pd.to_numeric(df_filtered['int_rate'], errors='coerce')\n",
        "\n",
        "# Handle potential NaNs in reward features before calculation\n",
        "reward_imputer = SimpleImputer(strategy='median')\n",
        "df_filtered[REWARD_FEATURES] = reward_imputer.fit_transform(df_filtered[REWARD_FEATURES])\n",
        "\n",
        "# Calculate profit (as a percentage of loan amount)\n",
        "df_filtered['profit_pct'] = df_filtered['int_rate'] / 100.0\n",
        "\n",
        "# Define reward (r)\n",
        "# r = 0 (Deny) is implied, we only care about the 'Approve' action\n",
        "# If action == Approve and Fully Paid (target=0): reward = + (loan_amnt * int_rate) -> We'll use profit_pct for simplicity\n",
        "# If action == Approve and Defaulted (target=1): reward = - loan_amnt -> We'll use -1 (100% loss of principal)\n",
        "df_filtered['reward'] = df_filtered.apply(\n",
        "    lambda row: row['profit_pct'] if row['target'] == 0 else -1.0,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(f\"Target variable 'target' created. Distribution:\\n{df_filtered['target'].value_counts(normalize=True)}\")\n",
        "print(f\"Reward variable 'reward' created. Mean reward: {df_filtered['reward'].mean():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSkrMYzmbP5C",
        "outputId": "c7ee1055-7f98-425d-d46e-79c1dbc2f63e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering for terminal loan statuses...\n",
            "Filtered size (terminal loans only): 176083 rows\n",
            "Engineering Target (y) and Reward (r) variables...\n",
            "Target variable 'target' created. Distribution:\n",
            "target\n",
            "0    0.800713\n",
            "1    0.199287\n",
            "Name: proportion, dtype: float64\n",
            "Reward variable 'reward' created. Mean reward: -0.1075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Build Preprocessing Pipeline ---\n",
        "print(\"Building preprocessing pipeline...\")\n",
        "\n",
        "# Numeric pipeline: Impute missing values with median, then scale\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical pipeline: Impute missing values with most frequent, then one-hot encode\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "# Combine pipelines using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, NUMERIC_FEATURES),\n",
        "        ('cat', categorical_transformer, CATEGORICAL_FEATURES)\n",
        "    ],\n",
        "    remainder='drop' # Drop any columns not specified\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXR7b-RQbSta",
        "outputId": "12c61ea0-9371-4dd8-f23d-30d4210520ed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building preprocessing pipeline...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# --- 7. Split, Process, and Save Data ---\n",
        "print(\"Splitting data into train and test sets...\")\n",
        "X = df_filtered[ALL_FEATURES]\n",
        "y = df_filtered['target']\n",
        "r = df_filtered['reward']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test, r_train, r_test = train_test_split(\n",
        "    X, y, r, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "# Fit the preprocessor on the training data\n",
        "print(\"Fitting preprocessor on training data...\")\n",
        "preprocessor.fit(X_train)\n",
        "\n",
        "# Define full save paths\n",
        "preprocessor_path = os.path.join(SAVE_DIR, 'preprocessor.joblib')\n",
        "train_data_path = os.path.join(SAVE_DIR, 'processed_data_train.npz')\n",
        "test_data_path = os.path.join(SAVE_DIR, 'processed_data_test.npz')\n",
        "\n",
        "# Save the fitted preprocessor\n",
        "joblib.dump(preprocessor, preprocessor_path)\n",
        "print(f\"Fitted preprocessor saved to '{preprocessor_path}'\")\n",
        "\n",
        "# Transform both train and test data\n",
        "print(\"Transforming train and test data...\")\n",
        "X_train_processed = preprocessor.transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Get feature names after transformation (for debugging/inspection)\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "print(f\"Data transformed. Total features after processing: {len(feature_names)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02KQ3LHlbVMr",
        "outputId": "6b5c1f4c-028d-40c8-c9c4-2319d1556f4f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting data into train and test sets...\n",
            "Training set size: 140866\n",
            "Test set size: 35217\n",
            "Fitting preprocessor on training data...\n",
            "Fitted preprocessor saved to '/content/drive/MyDrive/shodhAI/preprocessor.joblib'\n",
            "Transforming train and test data...\n",
            "Data transformed. Total features after processing: 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Save Processed Data for Models ---\n",
        "print(f\"Saving processed data to {SAVE_DIR}...\")\n",
        "# Save processed data in a compressed format\n",
        "np.savez_compressed(\n",
        "    train_data_path,\n",
        "    X=X_train_processed,\n",
        "    y=y_train.values,\n",
        "    r=r_train.values\n",
        ")\n",
        "np.savez_compressed(\n",
        "    test_data_path,\n",
        "    X=X_test_processed,\n",
        "    y=y_test.values,\n",
        "    r=r_test.values\n",
        ")\n",
        "\n",
        "print(\"\\n--- Preprocessing Task 1 Complete ---\")\n",
        "print(\"We now have the following files in your Google Drive /shodhAI/ folder:\")\n",
        "print(f\"1. '{preprocessor_path}' (our fitted pipeline)\")\n",
        "print(f\"2. '{train_data_path}' (for training models)\")\n",
        "print(f\"3. '{test_data_path}' (for evaluating models)\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KRFGiUPbWzk",
        "outputId": "def008d4-3f40-4a69-94ca-b02e0a3a182e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving processed data to /content/drive/MyDrive/shodhAI...\n",
            "\n",
            "--- Preprocessing Task 1 Complete ---\n",
            "We now have the following files in your Google Drive /shodhAI/ folder:\n",
            "1. '/content/drive/MyDrive/shodhAI/preprocessor.joblib' (our fitted pipeline)\n",
            "2. '/content/drive/MyDrive/shodhAI/processed_data_train.npz' (for training models)\n",
            "3. '/content/drive/MyDrive/shodhAI/processed_data_test.npz' (for evaluating models)\n"
          ]
        }
      ]
    }
  ]
}