{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu6Fh7Im_TFg",
        "outputId": "62eb24af-706c-4f9f-b7a0-138d4909201d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Test data loaded.\n",
            "Loading Task 2 (DL) model...\n",
            "Task 2 (DL) model loaded.\n",
            "Loading Task 3 (RL) model...\n",
            "Task 3 (RL) model loaded.\n",
            "\n",
            "--- Model Policy Comparison ---\n",
            "Task 2 (DL) Policy: Approves 21870/35217 loans (62.10%)\n",
            "Task 3 (RL) Policy: Approves 2385/35217 loans (6.77%)\n",
            "\n",
            "--- Finding Policy Disagreements ---\n",
            "Found 19485 loans where the DL model (Task 2) approves but the RL model (Task 3) denies.\n",
            "\n",
            "--- Analyzing 5 Example Disagreements ---\n",
            "\n",
            "--- Example 1 (Test Set Index: 0) ---\n",
            "  > True Outcome: Fully Paid\n",
            "  > True Reward:  $0.10\n",
            "\n",
            "  Task 2 (DL) Model:\n",
            "    Predicted P(Default): 0.3900\n",
            "    Decision: APPROVE (because 0.3900 < 0.516)\n",
            "\n",
            "  Task 3 (RL) Model:\n",
            "    Q-Value (Deny):    0.7597\n",
            "    Q-Value (Approve): 0.7136\n",
            "    Decision: DENY (because 0.7597 > 0.7136)\n",
            "\n",
            "  Analysis:\n",
            "    This is an applicant the DL model thinks is 'safe enough' (prob < 51.6%),\n",
            "    but the RL model, trained on profit, learned that the risk-vs-reward\n",
            "    for this type of applicant is a *bad financial bet*.\n",
            "\n",
            "--- Example 2 (Test Set Index: 2) ---\n",
            "  > True Outcome: Fully Paid\n",
            "  > True Reward:  $0.05\n",
            "\n",
            "  Task 2 (DL) Model:\n",
            "    Predicted P(Default): 0.0886\n",
            "    Decision: APPROVE (because 0.0886 < 0.516)\n",
            "\n",
            "  Task 3 (RL) Model:\n",
            "    Q-Value (Deny):    0.7893\n",
            "    Q-Value (Approve): 0.7883\n",
            "    Decision: DENY (because 0.7893 > 0.7883)\n",
            "\n",
            "  Analysis:\n",
            "    This is an applicant the DL model thinks is 'safe enough' (prob < 51.6%),\n",
            "    but the RL model, trained on profit, learned that the risk-vs-reward\n",
            "    for this type of applicant is a *bad financial bet*.\n",
            "\n",
            "--- Example 3 (Test Set Index: 4) ---\n",
            "  > True Outcome: Fully Paid\n",
            "  > True Reward:  $0.09\n",
            "\n",
            "  Task 2 (DL) Model:\n",
            "    Predicted P(Default): 0.3243\n",
            "    Decision: APPROVE (because 0.3243 < 0.516)\n",
            "\n",
            "  Task 3 (RL) Model:\n",
            "    Q-Value (Deny):    0.7801\n",
            "    Q-Value (Approve): 0.7409\n",
            "    Decision: DENY (because 0.7801 > 0.7409)\n",
            "\n",
            "  Analysis:\n",
            "    This is an applicant the DL model thinks is 'safe enough' (prob < 51.6%),\n",
            "    but the RL model, trained on profit, learned that the risk-vs-reward\n",
            "    for this type of applicant is a *bad financial bet*.\n",
            "\n",
            "--- Example 4 (Test Set Index: 5) ---\n",
            "  > True Outcome: Fully Paid\n",
            "  > True Reward:  $0.06\n",
            "\n",
            "  Task 2 (DL) Model:\n",
            "    Predicted P(Default): 0.1447\n",
            "    Decision: APPROVE (because 0.1447 < 0.516)\n",
            "\n",
            "  Task 3 (RL) Model:\n",
            "    Q-Value (Deny):    0.7894\n",
            "    Q-Value (Approve): 0.7824\n",
            "    Decision: DENY (because 0.7894 > 0.7824)\n",
            "\n",
            "  Analysis:\n",
            "    This is an applicant the DL model thinks is 'safe enough' (prob < 51.6%),\n",
            "    but the RL model, trained on profit, learned that the risk-vs-reward\n",
            "    for this type of applicant is a *bad financial bet*.\n",
            "\n",
            "--- Example 5 (Test Set Index: 7) ---\n",
            "  > True Outcome: Fully Paid\n",
            "  > True Reward:  $0.12\n",
            "\n",
            "  Task 2 (DL) Model:\n",
            "    Predicted P(Default): 0.5048\n",
            "    Decision: APPROVE (because 0.5048 < 0.516)\n",
            "\n",
            "  Task 3 (RL) Model:\n",
            "    Q-Value (Deny):    0.7692\n",
            "    Q-Value (Approve): 0.7003\n",
            "    Decision: DENY (because 0.7692 > 0.7003)\n",
            "\n",
            "  Analysis:\n",
            "    This is an applicant the DL model thinks is 'safe enough' (prob < 51.6%),\n",
            "    but the RL model, trained on profit, learned that the risk-vs-reward\n",
            "    for this type of applicant is a *bad financial bet*.\n",
            "\n",
            "--- Task 4 (Analysis Script) Complete ---\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from google.colab import drive\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "# --- 0. Setup ---\n",
        "\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# --- 1. Define Model Architectures ---\n",
        "\n",
        "\n",
        "# --- Model 1 (Task 2): DL Classifier ---\n",
        "class LoanDefaultClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, output_dim, dropout_rate):\n",
        "        super(LoanDefaultClassifier, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_dim, hidden_dim_1))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Dropout(dropout_rate))\n",
        "        layers.append(nn.Linear(hidden_dim_1, hidden_dim_2))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Dropout(dropout_rate))\n",
        "        if hidden_dim_3 > 0:\n",
        "            layers.append(nn.Linear(hidden_dim_2, hidden_dim_3))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "            layers.append(nn.Linear(hidden_dim_3, output_dim))\n",
        "        else:\n",
        "            layers.append(nn.Linear(hidden_dim_2, output_dim))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# --- Model 2 (Task 3): RL Q-Network ---\n",
        "class QPolicyNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim):\n",
        "        super(QPolicyNet, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim_1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(hidden_dim_1, hidden_dim_2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(hidden_dim_2, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# --- 2. Load Data and Models ---\n",
        "print(\"Mounting Google Drive...\")\n",
        "try:\n",
        "    \n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    =\n",
        "\n",
        "    DRIVE_MOUNT_POINT = '/content/drive/MyDrive/'\n",
        "    PROJECT_DIR = os.path.join(DRIVE_MOUNT_POINT, 'shodhAI')\n",
        "\n",
        "    # Load Test Data\n",
        "    TEST_DATA_PATH = os.path.join(PROJECT_DIR, 'processed_data_test.npz')\n",
        "    with np.load(TEST_DATA_PATH) as data:\n",
        "        X_test = data['X'].astype(np.float32)\n",
        "        y_test = data['y'].astype(np.float32) \n",
        "        r_test = data['r'].astype(np.float32) \n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n",
        "    INPUT_DIM = X_test.shape[1]\n",
        "\n",
        "    \n",
        "    PREPROCESSOR_PATH = os.path.join(PROJECT_DIR, 'preprocessor.joblib')\n",
        "    preprocessor = joblib.load(PREPROCESSOR_PATH)\n",
        "    \n",
        "    feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "    print(\"Test data loaded.\")\n",
        "\n",
        "    # --- Load Model 1 (Task 2) ---\n",
        "    print(\"Loading Task 2 (DL) model...\")\n",
        "\n",
        "   \n",
        "    DL_BEST_PARAMS = {\n",
        "        'hidden_1': 256,\n",
        "        'hidden_2': 128,\n",
        "        'hidden_3': 0, \n",
        "        'dropout': 0.4\n",
        "    }\n",
        "    # ---------------------\n",
        "\n",
        "    DL_MODEL_PATH = os.path.join(PROJECT_DIR, 'dl_model_weights.pth')\n",
        "\n",
        "    dl_model = LoanDefaultClassifier(\n",
        "        INPUT_DIM,\n",
        "        DL_BEST_PARAMS['hidden_1'],\n",
        "        DL_BEST_PARAMS['hidden_2'],\n",
        "        DL_BEST_PARAMS['hidden_3'],\n",
        "        1,\n",
        "        DL_BEST_PARAMS['dropout']\n",
        "    ).to(DEVICE)\n",
        "    dl_model.load_state_dict(torch.load(DL_MODEL_PATH, map_location=DEVICE))\n",
        "    dl_model.eval()\n",
        "    print(\"Task 2 (DL) model loaded.\")\n",
        "\n",
        "    # --- Load Model 2 (Task 3) ---\n",
        "    print(\"Loading Task 3 (RL) model...\")\n",
        "    RL_MODEL_PATH = os.path.join(PROJECT_DIR, 'rl_q_model.pth')\n",
        "    \n",
        "    rl_model = QPolicyNet(INPUT_DIM, 256, 128, 2).to(DEVICE)\n",
        "    rl_model.load_state_dict(torch.load(RL_MODEL_PATH, map_location=DEVICE))\n",
        "    rl_model.eval()\n",
        "    print(\"Task 3 (RL) model loaded.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading models or data: {e}\")\n",
        "\n",
        "    raise e\n",
        "\n",
        "# --- 3. Get Predictions from Both Models ---\n",
        "with torch.no_grad():\n",
        "    # Model 1 (DL) Predictions\n",
        "    dl_logits = dl_model(X_test_tensor)\n",
        "    dl_probs = torch.sigmoid(dl_logits).cpu().numpy().flatten()\n",
        "\n",
        "    # NOTE: You MUST update this threshold to match the \"Optimal Threshold\"\n",
        "   \n",
        "    DL_THRESHOLD = 0.5160\n",
        "    dl_decisions = (dl_probs >= DL_THRESHOLD).astype(int) # 1 = Predict Default, 0 = Predict Paid\n",
        "    # We flip this for a final policy: 0 = Deny, 1 = Approve\n",
        "    dl_policy = 1 - dl_decisions # 0=Deny, 1=Approve\n",
        "\n",
        "    # Model 2 (RL) Predictions\n",
        "    rl_q_values = rl_model(X_test_tensor).cpu().numpy()\n",
        "    rl_policy = np.argmax(rl_q_values, axis=1) # 0 = Deny, 1 = Approve\n",
        "\n",
        "print(\"\\n--- Model Policy Comparison ---\")\n",
        "dl_approves = np.sum(dl_policy == 1)\n",
        "rl_approves = np.sum(rl_policy == 1)\n",
        "\n",
        "print(f\"Task 2 (DL) Policy: Approves {dl_approves}/{len(dl_policy)} loans ({dl_approves/len(dl_policy)*100:.2f}%)\")\n",
        "print(f\"Task 3 (RL) Policy: Approves {rl_approves}/{len(rl_policy)} loans ({rl_approves/len(rl_policy)*100:.2f}%)\")\n",
        "\n",
        "\n",
        "# --- 4. Find Disagreements (Key Analysis for Report) ---\n",
        "print(\"\\n--- Finding Policy Disagreements ---\")\n",
        "\n",
        "\n",
        "dl_approves_rl_denies = (dl_policy == 1) & (rl_policy == 0)\n",
        "disagreement_indices = np.where(dl_approves_rl_denies)[0]\n",
        "\n",
        "print(f\"Found {len(disagreement_indices)} loans where the DL model (Task 2) approves but the RL model (Task 3) denies.\")\n",
        "\n",
        "# --- 5. Analyze the Disagreements ---\n",
        "print(\"\\n--- Analyzing 5 Example Disagreements ---\")\n",
        "\n",
        "disagreement_X = X_test[disagreement_indices]\n",
        "disagreement_y_true = y_test[disagreement_indices] # 1=Default, 0=Paid\n",
        "disagreement_r_true = r_test[disagreement_indices] # True *unscaled* profit/loss\n",
        "\n",
        "for i in range(min(5, len(disagreement_indices))):\n",
        "    idx_in_test_set = disagreement_indices[i]\n",
        "    print(f\"\\n--- Example {i+1} (Test Set Index: {idx_in_test_set}) ---\")\n",
        "\n",
        "    # Get model outputs for this one person\n",
        "    prob_default = dl_probs[idx_in_test_set]\n",
        "    q_deny = rl_q_values[idx_in_test_set, 0]\n",
        "    q_approve = rl_q_values[idx_in_test_set, 1]\n",
        "\n",
        "    print(f\"  > True Outcome: {'DEFAULTED' if disagreement_y_true[i] == 1 else 'Fully Paid'}\")\n",
        "    print(f\"  > True Reward:  ${disagreement_r_true[i]:.2f}\")\n",
        "\n",
        "    print(\"\\n  Task 2 (DL) Model:\")\n",
        "    print(f\"    Predicted P(Default): {prob_default:.4f}\")\n",
        "    print(f\"    Decision: APPROVE (because {prob_default:.4f} < {DL_THRESHOLD})\")\n",
        "\n",
        "    print(\"\\n  Task 3 (RL) Model:\")\n",
        "    print(f\"    Q-Value (Deny):    {q_deny:.4f}\")\n",
        "    print(f\"    Q-Value (Approve): {q_approve:.4f}\")\n",
        "    print(f\"    Decision: DENY (because {q_deny:.4f} > {q_approve:.4f})\")\n",
        "\n",
        "    print(\"\\n  Analysis:\")\n",
        "    print(\"    This is an applicant the DL model thinks is 'safe enough' (prob < 51.6%),\")\n",
        "    print(\"    but the RL model, trained on profit, learned that the risk-vs-reward\")\n",
        "    print(\"    for this type of applicant is a *bad financial bet*.\")\n",
        "\n",
        "    # You could also print the top 5 features for this applicant\n",
        "    # print(\"\\n  Applicant's (Scaled) Features:\")\n",
        "    # top_features_idx = np.argsort(disagreement_X[i])[-5:]\n",
        "    # for feat_idx in top_features_idx:\n",
        "    #     print(f\"    {feature_names[feat_idx]}: {disagreement_X[i, feat_idx]:.2f}\")\n",
        "\n",
        "print(\"\\n--- Task 4 (Analysis Script) Complete ---\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
